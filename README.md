# Tokenomics: The economics of deploying LLM chatbots

## 1. Introduction

- **Gen-AI for enterprises:**
	Breakthroughs in Generative AI hold the potential to transform productivity in the workplace by automating a large number of tasks involving Natural Language Understanding. Large Language Models (LLMs) can perform a number of tasks including document summarization, content creation, and power the next generation of intelligent chatbots.

- **Project background and motivation:**
	Intuition suggests that automating many of these critical tasks would improve productivity and lower costs, however, the benefits of LLMs need to be weight carefully against their costs. While key decision makers are excited by the potential gains promised by LLMs, they often lack a formal framework for putting a $value to the economic impact of deploying LLMs in an enterprise setting.

- **Key objective and contributions:**
	This project tackles the broad question of economic benefits of LLMs by attempting to put a $value to each token generated by the LLM enabling a direct comparison with the $cost of generating each token. Through this framework, we hope to empower business leaders to make more informed decisions about the financial benefits, if any, of deploying LLMs.

## 2. Methodology

- **Project scoping:** Our analysis focuses on the costs of deploying customer facing chatbots to automate parts of the customer service workflow. Of all the enterprise applications of LLMs, chatbots are perhaps the most ubiquitous with applications across industries and verticals.

- **Interviews:** We conducted multiple interviews with business leaders in the enterprise space as well as innovators who are building intelligent conversational agents. We used these insights to better understand success metrics and risks involved with chatbots.

- **Modeling:** Following the interviews, we were able to create a value model for chatbots to compare costs and potential benefits.

- **Testing:** We simulated a large number of conversations with a custom chatbot to gather aggregate metrics for our analysis.

- **Takeaways:** We highlighted our findings from the interviews, value model, and simulated conversations, and recommended a low-risk deployment strategy.

## 3. Interviews

- **Current adoption:** Enterprises are already adopting LLMs for internal use cases to enhance the productivity of their customer service staff. The primary application is querying the company's knowledge base to rapidly search for the policies relevant to customer queries. Retrieval Augmented Generation is a vital component of these systems.

- **Success metrics:** Representatives are evaluated based on their resolution rate and query time. A successful LLM powered chatbot deployment would require equivalent or better performance as measured by these metrics as well as user engagement and error rates.

- **Key challenges in deploying chatbots:** Adoption of automated solutions is challenging due to internal funding challenges and questions about long-term commitment. Risk averse organizations are as of yet unsure about how models can be constrained to provide helpful and factual answers. It is challenging to for LLM API providers to provide concrete guarantees about safety and helpfulness given the stochastic nature of these models. Concerns about privacy and security when using 3rd party APIs are also prevalent.

## 4. Tokenomics value model

### Costs of running a chatbot
- The primary focus is recurring costs which dominate over fixed costs in the long run
- Hidden costs of a poor customer experience are significant but harder to quantify
- Recurring costs are dominated by LLM API costs, for example OpenAI's pricing for ChatGPT

### LLM API costs
- A token is a word or part of a word. Text in the form of sentences or paragraphs is broken up into a sequence of tokens and fed into the LLM. The LLM iteratively generates a sequence of output tokens which constitute the response.
- API pricing is measured in terms of token usage. It is based on the number of input tokens and output tokens, for example an API call to GPT-4 Turbo costs $0.1 / 1000 input tokens and $0.3 / 1000 output tokens.

## System prompt
- A system prompt is typically a set of instructions provided to the LLM which define its scope and behavior. For instance, it could describe a personality, a response style, and desired functionality along with other metadata such as data and customer ID. This system prompt is always added to the input supplied to the API.
- The number of input tokens per API call will therefore always include a fixed overhead in terms of the number of tokens in the system prompt in addition to the actual query tokens.

### Conversational history
- LLM API calls don't store the history of previous calls. Each request is independent of the others.
- In a multi-turn conversation, a naively implemented chatbot would not be able to keep track of context if it only considers the immediate query from the user.
- Solution: The input supplied to the API contains not just the current query but also all the previous questions and answers up to the limit of the model's context size.
- This means that as the conversation progresses, the API calls tend to become more expensive until the context limit is hit.

### External context
- Sometimes, the LLM needs to refer to external data to accurately answer the customer's question. For instance, it may need to look up specific policies in the company's knowledge base using a RAG query, or it may need to look up order details in a tradition database to determine order status.
- The chatbot's response to the customer involves a multi-step process with one or more API calls behind the scenes to gather the relevant context.
- Step 1: In response to the immediate query, the chatbot's API call to the LLM generates an output which indicates a contextual query to a back end system.
- Step 2: This query, either to a vector database for RAG or to a traditional database, is executed and the response is made available to the chatbot.
- Step 3. The second API call adds this context to the user query and the LLM generates a grounded response to the query with the aid of the supplied context.

## 5. Simulated conversations and analysis

### Data generation

- **Creating a custom GPT:** We created a custom ChatGPT instance to play the role of a Walmart customer service chatbot.
- **Gathering simulated data:** Our team had multiple conversations with this GPT to simulate interactions with customers.
- **Aggregating results:** We analyzed the conversation transcripts to determine the average token usage statistics and costs of API calls.

### Cost-benefit analysis

The average cost per conversation using a chatbot can be broken down as follows:
- Average cost per conversation of = average number of API calls $\times$ average cost per API call.
- Average cost per API call = (average number of input tokens $\times$ cost per input token) + (average number of output tokens $\times$ cost per output token).

The average cost per conversation using a human can be calculated as follows:
- Average cost per conversation = Hourly wage / average number of conversations per hour.

**Safety costs:** These equations don't account for the costs of the automated safety mechanisms required to add guardrails to LLMs. While this cost is highly dependent on the implementation of the guardrails, even a simple approach such as having an LLM reflect on its proposed response to check if it correctly answers the question, is helpful and not toxic, etc., applies a ~2x cost multiplier. This multiplier could be higher for more critical applications or lower for lower stakes deployments such as internal chat systems.

## 6. Conclusion and key recommendations

### Summary
- There is no one size fits all answer for whether LLM chatbots offer an economic advantage compared to human workers.
- The tokenomics framework helps to formalize the calculations based on the aggregate statistics derived from historical customer support logs
- More powerful models like GPT-4 offer advantages in terms of safety and helpfulness and can also perform more sophisticated tasks such as querying backend systems, but come at a much greater cost.
- The cost of guardrails could add a significant multiplier depending on the nature of the deployment and the implementation of the safety systems.

### De-risking deployment
- Pilot studies involving LLMs as conversational agents should be designed as human-in-the-loop systems.
- Human representatives augmented by these systems have the option to directly pass through the chatbot response to the customer or correct it in case of undesirable behavior.
- This feedback system will both improve their productivity as well as generate vital alignment signals for RLHF or DPO, aiding in the continuous improvement of these models.

### Improving the customer support experience

**LLM powered chatbots offer multiple advantages in addition to potential cost savings**
1. Uniformity of experience: Using a single system standardizes responses and eliminates the inherent variability among human representatives.
2. Improvements made to the chatbot system result in improvements for all customers. In contrast, experience gained by human workers is not easily transferable.
3. LLM chatbots offer on-demand scalability to cater to spikes in traffic. For instance, a snowstorm resulting in cancelled flights could put undue stress on human workers. Augmenting them by spinning up LLM agents would help to meet the sudden influx of messages from stranded passengers.